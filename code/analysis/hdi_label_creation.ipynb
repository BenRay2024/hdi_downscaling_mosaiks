{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "repo_dir = os.environ.get(\"REPO_DIR\")\n",
    "code_dir = os.path.join(repo_dir, \"code/\")\n",
    "data_dir = os.path.join(repo_dir, \"data/\")\n",
    "\n",
    "os.chdir(code_dir)\n",
    "\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from mosaiks.label_utils.utils import geopandas_shape_grid, box_grid, assign_grid_points_to_gpdFile, get_dense_grid_for_gpdf_file\n",
    "from mosaiks.label_utils.plotting_utils import plot_label_map_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDI data (Smits et al)\n",
    "\n",
    "**Data Download:**\n",
    "\n",
    "*Shapefiles and tabular data are separate downloads*\n",
    "\n",
    "*Files downloaded on July 17, 2023:*\n",
    "\n",
    "Tabular data:\n",
    "https://globaldatalab.org/mygdl/downloads/\n",
    "\n",
    "\n",
    "https://globaldatalab.org/asset/394/SHDI-SGDI-Total%207.0.csv\n",
    "\n",
    "We are using the SHDI V7.0data in this analysis. The full database is downloaded from the link above. Version history is [here](https://globaldatalab.org/shdi/archive/).\n",
    "\n",
    "\n",
    "*A previous version of this manuscript used the V4 version of these labels.*\n",
    "\n",
    "\n",
    "Shapefiles:\n",
    "https://globaldatalab.org/shdi/shapefiles/\n",
    "\n",
    "https://globaldatalab.org/asset/403/GDL%20Shapefiles%20V6.1.zip\n",
    "\n",
    "We use the neweset shapefile available on July 17, 2023. This is the `GDL Shapefiles V6.1`. This file is NOT included in the GitHub repository and must be downloaded to replicate our data cleaning.\n",
    "\n",
    "\n",
    "\n",
    "**Data Citation**\n",
    "\n",
    "Smits, J., Permanyer, I. The Subnational Human Development Database. Sci Data 6, 190038 (2019). https://doi.org/10.1038/sdata.2019.38\n",
    "\n",
    "**Corresponding paper:**\n",
    "\n",
    "https://www.nature.com/articles/sdata201938\n",
    "\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "In this paper we describe the Subnational Human Development Database. This database contains for the period 1990–2017 for 1625 regions within 161 countries the national and subnational values of the Subnational Human Development Index (SHDI), for the three dimension indices on the basis of which the SHDI is constructed – education, health and standard of living --, and for the four indicators needed to create the dimension indices -- expected years of schooling, mean years of schooling, life expectancy and gross national income per capita. The subnational values of the four indicators were computed using data from statistical offices and from the Area Database of the Global Data Lab, which contains indicators aggregated from household surveys and census datasets. Values for missing years were estimated by interpolation and extrapolation from real data. By normalizing the population-weighted averages of the indicators to their national levels in the UNDP-HDI database, values of the SHDI and its dimension indices were obtained that at national level equal their official versions of the UNDP.\n",
    "\n",
    "\n",
    "**Data sources**\n",
    "\n",
    "Three major data sources were used to create our SHDI database. We approached statistical offices, including Eurostat, the statistical office of the European Union (https://ec.europa.eu/eurostat), by email communication or visiting their websites to obtain data. We downloaded data from the Area Database of the Global Data Lab (https://www.globaldatalab.org). And we downloaded data from the HDI website of the Human Development Report Office of the United Nations Development Program (http://hdr.undp.org). In the ‘SHDI Start’ data file (Data Citation 1), for each country information is provided on the data source(s) used for the subnational values of the indicators. In this file also for each country the years for which data is available, the number of subnational regions and the population size is presented. Below we discuss the three main data sources in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read in shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = data_dir + \"raw/GDL_HDI/\"\n",
    "out_directory = data_dir + \"int/GDL_HDI/\"\n",
    "\n",
    "## This file MUST be downloaded from the link above and placed in the correct subdirectory\n",
    "\n",
    "\n",
    "shp_path = directory+\"GDL_Shapefiles_V6.1/shdi2022_World_large.shp\"\n",
    "\n",
    "if os.path.exists(shp_path):\n",
    "    print(\"reading shp\")\n",
    "    gpdf = gpd.read_file(shp_path)\n",
    "    \n",
    "elif os.path.exists(directory+\"GDL Shapefiles V6.1.zip\"):\n",
    "    print(\"unzipping file\")\n",
    "    try:\n",
    "        os.mkdir(directory+\"/GDL_Shapefiles_V6.1/\")\n",
    "    except:\n",
    "        shutil.unpack_archive(directory+\"GDL Shapefiles V6.1.zip\", directory+\"/GDL_Shapefiles_V6.1\")\n",
    "else:\n",
    "    print(\"Shapefile needs to be downnloaded and placed in the correct directory. See details above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdf.rename(columns = {\"gdlcode\":\"GDLcode\"}, inplace=True) #Revert to an older name convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdf[gpdf[\"GDLcode\"].isnull()]  # No null GDLcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdf.set_index(\"GDLcode\", inplace=True)\n",
    "gpdf.loc[\"BHRt\",\"iso_code\"] = \"BHR\" # Fix weird anomaly in shapefile\n",
    "gpdf.loc[gpdf.index.str.startswith(\"CUB\"),\"iso_code\"] = \"CUB\" # fix missing iso code or Cuba\n",
    "gpdf[\"iso_code\"] = gpdf[\"iso_code\"].replace(\"XKO\",\"KSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = gpdf[gpdf[\"iso_code\"].isnull()] # Make a df of remaining null values in the country code\n",
    "\n",
    "gpdf.dropna(subset = [\"iso_code\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make and save a country aggregated version of this shapefiile -- it will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpdf_country = gpdf.dissolve(\"iso_code\")\n",
    "# gpdf_country.to_pickle(out_directory + \"/HDI_ADM0_dissolved_shapefile.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdf_country = pd.read_pickle(out_directory + \"/HDI_ADM0_dissolved_shapefile.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data files\n",
    "\n",
    "See above for details on this tabular data download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(directory + \"/SHDI-SGDI-Total 7.0.csv\",low_memory = False)\n",
    "\n",
    "#Subset to only 2019 observations. This is the year for which we have MOSAIKS features\n",
    "data = data[data[\"year\"] == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"GDLCODE\"].isin(nulls.index)] # None of the remaining null iso codes have matching HDI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dictionary = {\"shdi\" : \"Sub-national HDI\",\n",
    "                    \"msch\": \"Mean years schooling\",\n",
    "                    \"esch\":\"Expected years schooling\",\n",
    "                    \"lifexp\":\"Life expectancy\",\n",
    "                    \"gnic\": \"GNI per capita in thousands of US$ (2011 PPP)\",\n",
    "                    \"iso_code\": \"ISO_Code\"}\n",
    "\n",
    "tasks = list(rename_dictionary.values())[:-1]\n",
    "\n",
    "data.rename(columns = rename_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unneeded_cols = ['sgdi', 'shdif', 'shdim',\n",
    "       'healthindex', 'healthindexf', 'healthindexm', 'incindex', 'incindexf',\n",
    "       'incindexm', 'edindex', 'edindexf', 'edindexm', 'eschf',\n",
    "       'eschm', 'mschf', 'mschm', 'gnicf',\n",
    "       'gnicm', \"lgnic\", \"lgnicf\", \"lgnicm\", \"lifexpf\", \"lifexpm\"]\n",
    "\n",
    "data.drop(columns = unneeded_cols, inplace=True)\n",
    "\n",
    "data[\"ISO_Code\"] = data[\"ISO_Code\"].replace(\"XKO\",\"KSV\") # Set ISO code for Kosovo. For our use, first 3 of GDLcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    data[task] = pd.to_numeric(data[task], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_data_only_indices = data.groupby(\"ISO_Code\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we want to take the countries where we only have national data and merge those with the dataframe of subnational entities\n",
    "national_data_only_indices = data.groupby(\"ISO_Code\").size()==1\n",
    "national_data_only = data.groupby(\"ISO_Code\").first()[national_data_only_indices].reset_index()\n",
    "\n",
    "subnational_data_only = data[data[\"level\"] == \"Subnat\"]\n",
    "\n",
    "df = pd.concat([national_data_only, subnational_data_only])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's inspect the set of countries that do not have subnational province observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "print(\"Countries that do not have ADM1 child regions:\")\n",
    "national_data_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all very small countries and this appears to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shapefile is not a perfect match the tabular data\n",
    "\n",
    "Let's analyze what is missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's inspect the set of countries that cannot be linked to a shapefile primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nats_dropped = national_data_only[~national_data_only[\"GDLCODE\"].isin(gpdf.index)]\n",
    "nats_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These is a vry small country. Excluding this from our analysis seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, let's inspect the set of ADM1 polygons that cannot be linked to a shapefile primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subnats_dropped = subnational_data_only[~subnational_data_only.GDLCODE.isin(gpdf.index)]\n",
    "subnats_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subnats_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping these 47 subnational observations is the best we can do. Some appear quite reasonable (e.g., it probably doesn't make sense to consider Guadeloupe a part of France for the purpose of this analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see if there is any data in the shapefile that is missing from the tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape file obs that don't match tabular data\")\n",
    "\n",
    "gpdf[~gpdf.index.isin(df.GDLCODE)] # Just a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dropped = len(nats_dropped) + len(subnats_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go ahead and subset both of these files to the matching set of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"GDLCODE\", inplace=True)\n",
    "#gpdf.set_index(\"GDLcode\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_locs = df.index[df.index.isin(gpdf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[matching_locs]\n",
    "gpdf = gpdf.loc[matching_locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(out_directory + \"/HDI_indicators_and_indices_clean.p\")\n",
    "gpdf.to_pickle(out_directory + \"/HDI_ADM1_shapefile_clean.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also write the national level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_data = data[data[\"level\"] == \"National\"].set_index(\"ISO_Code\")\n",
    "nat_data.loc[gpdf_country.index] # Only include countries that also have a shapefile\n",
    "nat_data.to_pickle(out_directory + \"/HDI_indicators_and_indices_adm0_clean.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(n_dropped/(len(matching_locs ) + n_dropped),3) * 100, \"% of HDI data dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transform shapefile to .01 x . 01 degree grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the form needed for aggregating features in the existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_grid = get_dense_grid_for_gpdf_file(gpdf.reset_index(), columns=[\"GDLCODE\", \"iso_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpath = data_dir + \"/features/prepared_labels/GDL_HDI_polygon_coords_for_featurization.p\"\n",
    "# dense_grid[\"constant\"] = 1\n",
    "\n",
    "\n",
    "# dense_grid.to_pickle(outpath)\n",
    "# dense_grid = pd.read_pickle(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if any polygon observations were dropped. This would occur if they are very small and don't overlay any grid centorids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dense_grid[\"GDLCODE\"].unique()) == len(matching_locs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
